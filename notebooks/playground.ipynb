{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A place to test random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u1910100/miniconda3/envs/tiatoolbox/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from monkey.model.utils import (\n",
    "    get_classification_metrics,\n",
    "    get_activation_function,\n",
    ")\n",
    "from monkey.model.loss_functions import get_loss_function, dice_coeff\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import segmentation_models_pytorch as smp\n",
    "from monkey.data.data_utils import generate_regression_map\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "          [0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "          [0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_map\n",
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0343, 0.1688, 0.2302, 0.1688, 0.0343,\n",
      "           0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.1688, 0.4054, 0.5516, 0.4054, 0.1688,\n",
      "           0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.2302, 0.5516, 1.0000, 0.5516, 0.2302,\n",
      "           0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.1688, 0.4054, 0.5516, 0.4054, 0.1688,\n",
      "           0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0343, 0.1688, 0.2302, 0.1688, 0.0343,\n",
      "           0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000]]]], dtype=torch.float64)\n",
      "weighted bce loss\n",
      "tensor(2.7771, dtype=torch.float64)\n",
      "bce loss\n",
      "tensor(2.4863, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = get_loss_function('Weighted_BCE_Jaccard')\n",
    "\n",
    "bce_loss_fn = get_loss_function('Weighted_BCE')\n",
    "# loss_fn.set_alpha(0.5)\n",
    "# focal_loss_fn = smp.losses.FocalLoss(mode='binary', alpha=0.25, gamma=2, reduction=None)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_true = torch.zeros(size=(1,1,9,9))\n",
    "    y_true[0,0,4:7,4:7] = 1\n",
    "    print(y_true)\n",
    "    plt.imshow(y_true[0,0])\n",
    "    plt.show()\n",
    "\n",
    "    centroid_map = np.zeros((9,9))\n",
    "    centroid_map[5,5] = 1\n",
    "    weight_map = generate_regression_map(centroid_map, d_thresh=3, alpha=1, scale=1)\n",
    "    plt.imshow(weight_map)\n",
    "    plt.show()\n",
    "\n",
    "    weight_map = torch.tensor(weight_map).unsqueeze(0).unsqueeze(0)\n",
    "    print(\"weight_map\")\n",
    "    print(weight_map)\n",
    "\n",
    "    y_pred = torch.zeros(size=(1,1,9,9))\n",
    "    y_pred[0,0,4:7,4:7] = 0.5\n",
    "    y_pred[0,0,5,5] = 0.9\n",
    "    # print(y_pred)\n",
    "\n",
    "    loss = loss_fn.compute_loss(y_true, y_pred, weight_map)\n",
    "    bce_loss = bce_loss_fn.compute_loss(y_true, y_pred, weight_map)\n",
    "    print(\"weighted bce loss\")\n",
    "    print(loss)\n",
    "    print(\"bce loss\")\n",
    "    print(bce_loss)\n",
    "    # weighted_loss = loss * weight_map\n",
    "    # print(\"weighted loss\")\n",
    "    # print(weighted_loss)\n",
    "\n",
    "    # print(loss.mean())\n",
    "    # print(weighted_loss.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conch.open_clip_custom import (\n",
    "    create_model_from_pretrained,\n",
    "    tokenize,\n",
    "    get_tokenizer,\n",
    ")\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = \"conch_ViT-B-16\"\n",
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "checkpoint_path = \"/home/u1910100/Downloads/pytorch_model.bin\"\n",
    "model, preprocess = create_model_from_pretrained(\n",
    "    model_cfg, checkpoint_path, device=device\n",
    ")\n",
    "_ = model.eval()\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "classes = [\"lymphocyte\", \"monocyte\"]\n",
    "prompts = [\n",
    "    \"a PAS stained image of a lymphocyte\",\n",
    "    \"a PAS stained image of a monocyte\",\n",
    "]\n",
    "\n",
    "tokenized_prompts = tokenize(texts=prompts, tokenizer=tokenizer).to(\n",
    "    device\n",
    ")\n",
    "tokenized_prompts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monkey.data.dataset import get_classification_dataloaders\n",
    "from monkey.config import TrainingIOConfig\n",
    "\n",
    "IOconfig = TrainingIOConfig(\n",
    "    dataset_dir=\"/home/u1910100/Documents/Monkey/classification\",\n",
    "    save_dir=\"./\",\n",
    ")\n",
    "IOconfig.set_image_dir(\n",
    "    \"/home/u1910100/Documents/Monkey/classification/patches\"\n",
    ")\n",
    "IOconfig.set_mask_dir(\n",
    "    \"/home/u1910100/Documents/Monkey/classification/patches\"\n",
    ")\n",
    "batch_size = 32\n",
    "train_loader, val_loader = get_classification_dataloaders(\n",
    "    IOconfig,\n",
    "    val_fold=1,\n",
    "    batch_size=batch_size,\n",
    "    do_augmentation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "for data in tqdm(val_loader):\n",
    "    file_ids = data[\"id\"]\n",
    "\n",
    "    images, true_labels = (\n",
    "        data[\"image\"].cuda().float(),\n",
    "        data[\"label\"].cpu().tolist(),\n",
    "    )\n",
    "\n",
    "    true_labels_list.extend(true_labels)\n",
    "    pred_probs = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        image_embedings = model.encode_image(images)\n",
    "        text_embedings = model.encode_text(tokenized_prompts)\n",
    "        sim_scores = (\n",
    "            (\n",
    "                image_embedings\n",
    "                @ text_embedings.T\n",
    "                * model.logit_scale.exp()\n",
    "            )\n",
    "            .softmax(dim=-1)\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        pred_probs_list.extend(sim_scores[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from monkey.model.utils import get_classification_metrics\n",
    "\n",
    "pred_probs_list = np.array(pred_probs_list)\n",
    "true_labels_list = np.array(true_labels_list)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(\n",
    "    true_labels_list, pred_probs_list\n",
    ")\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display = metrics.RocCurveDisplay(\n",
    "    fpr=fpr,\n",
    "    tpr=tpr,\n",
    "    roc_auc=roc_auc,\n",
    "    estimator_name=\"cell classifier\",\n",
    ")\n",
    "display.plot()\n",
    "plt.show()\n",
    "\n",
    "thresh = 0.5\n",
    "pred_labels_list = np.where(pred_probs_list > thresh, 1, 0)\n",
    "scores = get_classification_metrics(\n",
    "    true_labels_list, pred_labels_list\n",
    ")\n",
    "print(scores)\n",
    "metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "    true_labels_list,\n",
    "    pred_labels_list,\n",
    "    display_labels=[\"lymphocyte\", \"monocyte\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(val_loader))\n",
    "images = data[\"image\"].to(\"cuda\").float()\n",
    "print(data[\"label\"])\n",
    "\n",
    "with torch.inference_mode():\n",
    "    image_embedings = model.encode_image(images)\n",
    "    text_embedings = model.encode_text(tokenized_prompts)\n",
    "    sim_scores = (\n",
    "        (image_embedings @ text_embedings.T * model.logit_scale.exp())\n",
    "        .softmax(dim=-1)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "pred_class = sim_scores.argmax()\n",
    "print(pred_class)\n",
    "print(sim_scores)\n",
    "print(sim_scores[:, 1])\n",
    "# print(\"Predicted class:\", classes[sim_scores.argmax()])\n",
    "# print(\n",
    "#     \"Normalized similarity scores:\",\n",
    "#     [\n",
    "#         f\"{cls}: {score:.3f}\"\n",
    "#         for cls, score in zip(classes, sim_scores[0])\n",
    "#     ],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CellViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import torch\n",
    "from monkey.model.cellvit.cellvit import CellViT256, CellVit256_Unet\n",
    "\n",
    "model_path = \"/home/u1910100/Downloads/HIPT_vit256_small_dino.pth\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = CellVit256_Unet(num_decoders=3)\n",
    "\n",
    "model.load_pretrained_encoder(model_path)\n",
    "\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "test = torch.rand(size=(4, 3, 256, 256))\n",
    "out = model(test)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapDe Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from scipy import ndimage, signal\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from monkey.data.data_utils import erode_mask, generate_regression_map\n",
    "import cv2\n",
    "from skimage import draw\n",
    "\n",
    "cell_mask = np.zeros(shape=(256, 256), dtype=np.uint8)\n",
    "rr, cc = draw.ellipse(100, 100, 1, 1, shape=cell_mask.shape)\n",
    "# rr, cc = 100, 100\n",
    "cell_mask[rr, cc] = 1\n",
    "plt.imshow(cell_mask)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def gauss_2d_filter(shape=(11, 11)):\n",
    "    sigma = int((shape[0] - 1) / 6)\n",
    "    m, n = [(ss - 1.0) / 2.0 for ss in shape]\n",
    "    y, x = np.ogrid[-m : m + 1, -n : n + 1]\n",
    "    h = np.exp(-(x * x + y * y) / (2.0 * sigma * sigma))\n",
    "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "\n",
    "    h = h / (h[int(m), int(n)])\n",
    "    return h\n",
    "\n",
    "\n",
    "dist_filter = gauss_2d_filter(shape=(17, 17))\n",
    "plt.imshow(dist_filter)\n",
    "plt.show()\n",
    "# cell_mask = generate_regression_map(cell_mask, d_thresh=5, alpha=0.5, scale=1)\n",
    "# plt.imshow(cell_mask)\n",
    "# plt.show()\n",
    "\n",
    "cell_mask = signal.convolve2d(cell_mask, dist_filter)\n",
    "print(np.max(cell_mask))\n",
    "plt.imshow(cell_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multihead Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from pprint import pprint\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "from torchvision.models.efficientnet import (\n",
    "    efficientnet_b0,\n",
    ")\n",
    "from monkey.model.efficientunetb0.architecture import (\n",
    "    get_multihead_efficientunet,\n",
    ")\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "model = get_multihead_efficientunet(\n",
    "    out_channels=[2, 1], pretrained=True\n",
    ")\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "summary(model, input_size=(1, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_input = torch.ones(\n",
    "        size=(1, 3, 256, 256), dtype=torch.float, device=\"cuda\"\n",
    "    )\n",
    "    model_out = model(test_input)\n",
    "    pprint(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapDe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from monkey.model.mapde.model import MapDe\n",
    "from monkey.data.dataset import get_detection_dataloaders\n",
    "from monkey.config import TrainingIOConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from monkey.data.data_utils import imagenet_denormalise\n",
    "import torch\n",
    "\n",
    "use_nuclick_masks = False\n",
    "batch_size = 1\n",
    "module = \"detection\"\n",
    "\n",
    "IOconfig = TrainingIOConfig(\n",
    "    dataset_dir=\"/home/u1910100/Documents/Monkey/patches_256\",\n",
    "    save_dir=\"./\",\n",
    ")\n",
    "\n",
    "\n",
    "train_loader, val_loader = get_detection_dataloaders(\n",
    "    IOconfig,\n",
    "    val_fold=5,\n",
    "    dataset_name=\"detection\",\n",
    "    batch_size=batch_size,\n",
    "    disk_radius=1,\n",
    "    regression_map=False,\n",
    "    do_augmentation=True,\n",
    "    module=module,\n",
    "    use_nuclick_masks=use_nuclick_masks,\n",
    "    include_background_channel=False,\n",
    ")\n",
    "\n",
    "\n",
    "model = MapDe(3, 30, 50, num_classes=1, filter_size=31)\n",
    "model.eval()\n",
    "# print(model)\n",
    "test_input = torch.ones(size=(4, 3, 252, 252), dtype=torch.float)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(test_input)\n",
    "print(f\"output size = {out.size()}\")\n",
    "# print(torch.max(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 18))\n",
    "\n",
    "image = data[\"image\"][0].numpy()\n",
    "image = np.moveaxis(image, 0, 2)\n",
    "image = imagenet_denormalise(image)\n",
    "axes[0].imshow(image)\n",
    "\n",
    "mask = data[\"mask\"]\n",
    "mask_filtered = model.blur_cell_points(mask)\n",
    "\n",
    "\n",
    "axes[1].imshow(image, alpha=0.5)\n",
    "mask_filtered_numpy = mask_filtered.numpy()\n",
    "axes[1].imshow(mask_filtered_numpy[0][0], alpha=0.5)\n",
    "axes[2].imshow(mask_filtered_numpy[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits = torch.rand(size=(2, 3, 252, 252))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(test_logits)\n",
    "    probs = model.logits_to_probs(logits)\n",
    "print(logits)\n",
    "\n",
    "out_masks = model.postproc(logits)\n",
    "out_masks = out_masks[:, np.newaxis, :, :]\n",
    "out_masks = model.blur_cell_points(out_masks)\n",
    "\n",
    "logits = logits.numpy(force=True)\n",
    "probs = probs.numpy(force=True)\n",
    "\n",
    "\n",
    "plt.imshow(logits[0][0])\n",
    "plt.show()\n",
    "print(np.max(logits[0][0]))\n",
    "\n",
    "plt.imshow(probs[0][0])\n",
    "plt.show()\n",
    "\n",
    "# print(np.max(logits))\n",
    "plt.imshow(out_masks[0][0])\n",
    "plt.show()\n",
    "\n",
    "import skimage\n",
    "\n",
    "inflamm_labels = skimage.measure.label(out_masks[0][0])\n",
    "inflamm_stats = skimage.measure.regionprops(\n",
    "    inflamm_labels, intensity_image=probs[0][0]\n",
    ")\n",
    "for region in inflamm_stats:\n",
    "    centroid = region[\"centroid\"]\n",
    "\n",
    "    c, r, confidence = (\n",
    "        centroid[1],\n",
    "        centroid[0],\n",
    "        region[\"mean_intensity\"],\n",
    "    )\n",
    "    print(c, r, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HoverNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from monkey.model.hovernext.model import (\n",
    "    get_custom_hovernext,\n",
    "    get_convnext_unet,\n",
    "    get_timm_encoder,\n",
    "    load_encoder_weights,\n",
    ")\n",
    "\n",
    "model = get_custom_hovernext(\n",
    "    enc=\"tf_efficientnetv2_m.in21k\",\n",
    "    pretrained=True,\n",
    "    num_heads=3,\n",
    "    decoders_out_channels=[3, 3, 3],\n",
    "    attention_type='scse',\n",
    "    use_batchnorm=True,\n",
    ")\n",
    "# model = get_convnext_unet(\n",
    "#     out_classes=2, use_batchnorm=True, attention_type=\"scse\"\n",
    "# )\n",
    "# checkpoint_path = \"/home/u1910100/Downloads/lizard_convnextv2_large/train/best_model\"\n",
    "# model = load_encoder_weights(model, checkpoint_path)\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "pprint(summary(model, input_size=(1, 3, 256, 256)))\n",
    "pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "test_input = torch.ones(\n",
    "    size=(1, 3, 256, 256), dtype=torch.float, device=\"cuda\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(test_input)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pannuke Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from monkey.data.data_utils import (\n",
    "    generate_regression_map,\n",
    "    dilate_mask,\n",
    "    load_image,\n",
    "    load_mask,\n",
    ")\n",
    "from monkey.data.dataset import class_mask_to_multichannel_mask\n",
    "\n",
    "cell_mask = np.zeros(shape=(256, 256), dtype=np.uint8)\n",
    "cell_mask[100, 100] = 1\n",
    "\n",
    "cell_mask = dilate_mask(cell_mask, 3)\n",
    "\n",
    "plt.imshow(cell_mask)\n",
    "plt.show()\n",
    "\n",
    "cell_mask = generate_regression_map(\n",
    "    cell_mask, d_thresh=7, alpha=3, scale=1\n",
    ")\n",
    "plt.imshow(cell_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = \"/home/u1910100/Documents/Monkey/patches_256/annotations/masks/C_P000029_18592_66528_18848_66784.npy\"\n",
    "mask = np.load(mask_path)\n",
    "class_mask = class_mask_to_multichannel_mask(mask)\n",
    "\n",
    "plt.imshow(class_mask[0])\n",
    "plt.show()\n",
    "plt.imshow(class_mask[1])\n",
    "plt.show()\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "dist = ndi.distance_transform_edt(class_mask[0] == 0)\n",
    "M = (np.exp(3 * (1 - dist / 7)) - 1) / (np.exp(3) - 1)\n",
    "M[M < 0] = 0\n",
    "M *= 1\n",
    "\n",
    "plt.imshow(M)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from strong_augment import StrongAugment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trnsf = T.Compose(\n",
    "    [\n",
    "        StrongAugment(\n",
    "            operations=[2, 3, 4], probabilites=[0.5, 0.3, 0.2]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the images\n",
    "image = np.load(\n",
    "    \"/home/u1910100/Documents/Monkey/patches_256/images/A_P000001_9632_87360_9888_87616.npy\"\n",
    ")\n",
    "aug_image = trnsf(image)\n",
    "\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Display the original image\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# image = image / 255.0\n",
    "# Display the augmented image\n",
    "axes[1].imshow(\n",
    "    aug_image\n",
    ")  # Permute the dimensions for correct display\n",
    "axes[1].set_title(\"Augmented Image\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from monkey.model.hovernext.modified_self_attention import (\n",
    "    get_model,\n",
    ")\n",
    "\n",
    "model = get_model(\n",
    "    enc=\"convnextv2_tiny.fcmae_ft_in22k_in1k\",\n",
    "    pretrained=False,\n",
    ")\n",
    "pprint(summary(model, input_size=(1, 3, 256, 256)))\n",
    "\n",
    "test_input = torch.ones(\n",
    "    size=(1, 3, 256, 256), dtype=torch.float, device=\"cuda\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    out = model(test_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from monkey.model.hovernext.modified_model import (\n",
    "    get_modified_hovernext\n",
    ")\n",
    "\n",
    "\n",
    "model = get_modified_hovernext(\"convnextv2_tiny.fcmae_ft_in22k_in1k\", pretrained=False)\n",
    "model.to(\"cuda\")\n",
    "test_input = torch.ones(\n",
    "    size=(1, 3, 256, 256), dtype=torch.float, device=\"cuda\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    out = model(test_input)\n",
    "print(out.shape)\n",
    "print(summary(model, input_size=(1, 3, 256, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process NuClick Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "NUCLICK_DIR = \"/home/u1910100/Documents/Monkey/patches_256/annotations/nuclick_hovernext\"\n",
    "\n",
    "SAVE_DIR = \"/home/u1910100/Documents/Monkey/patches_256/annotations/nuclick_masks_processed_v2\"\n",
    "\n",
    "\n",
    "def process_instance_and_class_map(instance_map, class_map):\n",
    "    # get initial binary mask from instance map\n",
    "    binary_mask = np.zeros(shape=(instance_map.shape), dtype=np.uint8)\n",
    "    binary_mask = np.where(instance_map > 0, 1, 0).astype(np.uint8)\n",
    "\n",
    "    lymph_mask = np.where(class_map == 1, 1, 0).astype(np.uint8)\n",
    "    mono_mask = np.where(class_map == 2, 1, 0).astype(np.uint8)\n",
    "\n",
    "    # Erode binary mask by gradient map\n",
    "    sx = ndimage.sobel(instance_map, axis=0)\n",
    "    sy = ndimage.sobel(instance_map, axis=1)\n",
    "    inflamm_gradient = np.hypot(sx, sy)\n",
    "    inflamm_gradient = (inflamm_gradient > 0).astype(np.uint8)\n",
    "    binary_mask[inflamm_gradient == 1] = 0\n",
    "\n",
    "    # Use instance mask to separate lymph and mono instances\n",
    "    lymph_mask[binary_mask == 0] = 0\n",
    "    mono_mask[binary_mask == 0] = 0\n",
    "\n",
    "    # Erode lymph_mask by gradient map\n",
    "    sx = ndimage.sobel(lymph_mask, axis=0)\n",
    "    sy = ndimage.sobel(lymph_mask, axis=1)\n",
    "    lymph_gradient = np.hypot(sx, sy)\n",
    "    lymph_gradient = (lymph_gradient > 0).astype(np.uint8)\n",
    "    lymph_mask[lymph_gradient == 1] = 0\n",
    "\n",
    "    # Erode mono_mask by gradient map\n",
    "    sx = ndimage.sobel(mono_mask, axis=0)\n",
    "    sy = ndimage.sobel(mono_mask, axis=1)\n",
    "    mono_gradient = np.hypot(sx, sy)\n",
    "    mono_gradient = (mono_gradient > 0).astype(np.uint8)\n",
    "    mono_mask[mono_gradient == 1] = 0\n",
    "\n",
    "    return {\n",
    "        \"inflamm_mask\": binary_mask,\n",
    "        \"inflamm_contour_mask\": inflamm_gradient,\n",
    "        \"lymph_mask\": lymph_mask,\n",
    "        \"lymph_contour_mask\": lymph_gradient,\n",
    "        \"mono_mask\": mono_mask,\n",
    "        \"mono_contour_mask\": mono_gradient,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_nuclick_data_file(file_name):\n",
    "    data_path = os.path.join(NUCLICK_DIR, file_name)\n",
    "    data = np.load(data_path)\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    image = data[:, :, 0:3]\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    instance_map = data[:, :, 3]\n",
    "    class_map = data[:, :, 4]\n",
    "\n",
    "    processed_masks = process_instance_and_class_map(\n",
    "        instance_map, class_map\n",
    "    )\n",
    "\n",
    "    new_data = np.zeros(\n",
    "        shape=(data.shape[0], data.shape[1], 9), dtype=np.uint8\n",
    "    )\n",
    "    new_data[:, :, 0:3] = image\n",
    "    new_data[:, :, 3] = processed_masks[\"inflamm_mask\"]\n",
    "    new_data[:, :, 4] = processed_masks[\"inflamm_contour_mask\"]\n",
    "    new_data[:, :, 5] = processed_masks[\"lymph_mask\"]\n",
    "    new_data[:, :, 6] = processed_masks[\"lymph_contour_mask\"]\n",
    "    new_data[:, :, 7] = processed_masks[\"mono_mask\"]\n",
    "    new_data[:, :, 8] = processed_masks[\"mono_contour_mask\"]\n",
    "\n",
    "    # save_path = os.path.join(SAVE_DIR, file_name)\n",
    "    # np.save(save_path, new_data)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "files = os.listdir(NUCLICK_DIR)\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    new_data = process_nuclick_data_file(file)\n",
    "\n",
    "    # Plot new data\n",
    "    fig, axes = plt.subplots(1, 7, figsize=(18, 18))\n",
    "    axes[0].imshow(new_data[:, :, 0:3])\n",
    "    axes[0].title.set_text(\"Image\")\n",
    "    axes[1].imshow(new_data[:, :, 3])\n",
    "    axes[1].title.set_text(\"Inflammation Mask\")\n",
    "    axes[2].imshow(new_data[:, :, 4])\n",
    "    axes[2].title.set_text(\"Inflammation Contour Mask\")\n",
    "    axes[3].imshow(new_data[:, :, 5])\n",
    "    axes[3].title.set_text(\"Lymph Mask\")\n",
    "    axes[4].imshow(new_data[:, :, 6])\n",
    "    axes[4].title.set_text(\"Lymph Contour Mask\")\n",
    "    axes[5].imshow(new_data[:, :, 7])\n",
    "    axes[5].title.set_text(\"Mono Mask\")\n",
    "    axes[6].imshow(new_data[:, :, 8])\n",
    "    axes[6].title.set_text(\"Mono Contour Mask\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiatoolbox.wsicore.wsireader import WSIReader\n",
    "\n",
    "wsi_path = \"/home/u1910100/Documents/Monkey/test/input/images/kidney-transplant-biopsy-wsi-pas/A_P000002_PAS_CPG.tif\"\n",
    "mask_path = \"/home/u1910100/Documents/Monkey/test/input/images/tissue-mask/A_P000002_mask.tif\"\n",
    "\n",
    "wsi_reader = WSIReader.open(wsi_path)\n",
    "mask_reader = WSIReader.open(mask_path)\n",
    "\n",
    "wsi_shape = wsi_reader.slide_dimensions(resolution=0, units=\"level\")\n",
    "mask_shape = mask_reader.slide_dimensions(resolution=0, units=\"level\")\n",
    "\n",
    "print(wsi_shape, mask_shape)\n",
    "\n",
    "print(mask_reader.info.as_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiatoolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
