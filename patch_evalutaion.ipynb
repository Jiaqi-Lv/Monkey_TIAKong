{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from monkey.model.efficientunetb0.architecture import (\n",
    "    get_efficientunet_b0_MBConv,\n",
    ")\n",
    "import skimage\n",
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "from monkey.config import TrainingIOConfig\n",
    "from monkey.data.dataset import get_dataloaders\n",
    "from monkey.data.data_utils import (\n",
    "    imagenet_denormalise,\n",
    "    load_json_annotation,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "import skimage\n",
    "from evaluation.evaluate import match_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_centers(cell_mask):\n",
    "    mask_label = skimage.measure.label(cell_mask)\n",
    "    stats = skimage.measure.regionprops(mask_label)\n",
    "    centers = []\n",
    "    for region in stats:\n",
    "        centroid = region[\"centroid\"]\n",
    "        centers.append(centroid)\n",
    "    return centers\n",
    "\n",
    "\n",
    "SPACING_LEVEL0 = 0.24199951445730394\n",
    "\n",
    "\n",
    "def evaluate_cell_predictions(gt_centers, pred_centers):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    result_prob = [1.0 for i in range(len(pred_centers))]\n",
    "    (\n",
    "        tp,\n",
    "        fn,\n",
    "        fp,\n",
    "        tp_probs,\n",
    "        fp_probs,\n",
    "    ) = match_coordinates(\n",
    "        gt_centers,\n",
    "        pred_centers,\n",
    "        result_prob,\n",
    "        int(7.5 / SPACING_LEVEL0),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1\n",
    "    try:\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 1\n",
    "\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def erode_mask(mask):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask = cv2.erode(mask, kernel, iterations=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_efficientunet_b0_MBConv(pretrained=False)\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"mit_b5\",\n",
    "#     encoder_weights=None,\n",
    "#     decoder_attention_type=\"scse\",\n",
    "#     in_channels=3,\n",
    "#     classes=1,\n",
    "# )\n",
    "\n",
    "val_fold = 4\n",
    "\n",
    "checkpoint_path = f\"/home/u1910100/Documents/Monkey/runs/efficientunetb0_seg/fold_{val_fold}/epoch_50.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(\"cuda\")\n",
    "\n",
    "IOconfig = TrainingIOConfig(\n",
    "    dataset_dir=\"/home/u1910100/Documents/Monkey/patches_256\",\n",
    "    save_dir=f\"./\",\n",
    ")\n",
    "\n",
    "# Get dataloaders for task\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    IOconfig,\n",
    "    val_fold=val_fold,\n",
    "    task=1,\n",
    "    batch_size=1,\n",
    "    disk_radius=11,\n",
    "    do_augmentation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "thresholds = [0.9]\n",
    "best_thresh = thresholds[0]\n",
    "best_F1 = 0\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"threshold {thresh}\")\n",
    "    counter = 0\n",
    "    sum_F1 = []\n",
    "    sum_precison = []\n",
    "    sum_recall = []\n",
    "    for data in tqdm(val_loader):\n",
    "        file_ids = data[\"id\"]\n",
    "\n",
    "        annotation_dict = load_json_annotation(file_ids[0], IOconfig)\n",
    "        images = data[\"image\"].cuda().float()\n",
    "        gt_masks = data[\"mask\"].cuda().float()\n",
    "\n",
    "        image_np = images[0]\n",
    "        image_np = image_np.cpu().numpy()\n",
    "        gt_mask_np = gt_masks[0]\n",
    "        gt_mask_np = gt_mask_np.cpu().numpy()\n",
    "\n",
    "        image_np = np.moveaxis(image_np, 0, 2)\n",
    "        image_np = imagenet_denormalise(image_np)\n",
    "\n",
    "        # fig, axs = plt.subplots(1, 4, figsize=(10,10))\n",
    "        # axs[0].imshow(image_np)\n",
    "        # axs[0].title.set_text(\"Image\")\n",
    "        # axs[1].imshow(gt_mask_np[0], cmap='gray')\n",
    "        # axs[1].title.set_text(\"Ground Truth\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(images)\n",
    "            out = torch.sigmoid(out)\n",
    "\n",
    "        out = out.cpu().detach().numpy()[0][0]\n",
    "\n",
    "        # axs[3].imshow(out, cmap='jet')\n",
    "\n",
    "        out_mask = np.where(out >= thresh, 1, 0).astype(np.uint8)\n",
    "        # out_mask = erode_mask(out_mask)\n",
    "\n",
    "        out_mask = skimage.morphology.remove_small_objects(\n",
    "            ar=out_mask, min_size=32\n",
    "        )\n",
    "\n",
    "        pred_centers = get_cell_centers(out_mask)\n",
    "        true_centers = get_cell_centers(gt_mask_np[0])\n",
    "        f1, precision, recall = evaluate_cell_predictions(\n",
    "            true_centers, pred_centers\n",
    "        )\n",
    "\n",
    "        # axs[2].imshow(out_mask, cmap=\"gray\")\n",
    "        # axs[2].title.set_text(\"Prediction\")\n",
    "\n",
    "        # for ax in fig.axes:\n",
    "        #     ax.axis(\"off\")\n",
    "        # plt.show()\n",
    "\n",
    "        sum_F1.append(f1)\n",
    "        sum_precison.append(precision)\n",
    "        sum_recall.append(recall)\n",
    "\n",
    "        # counter +=1\n",
    "        # if counter > 20:\n",
    "        #     break\n",
    "\n",
    "    print(\"Avg F1 \", np.mean(sum_F1))\n",
    "    print(\"Avg Precision \", np.mean(sum_precison))\n",
    "    print(\"Avg Recall \", np.mean(sum_recall))\n",
    "\n",
    "    if np.mean(sum_F1) > best_F1:\n",
    "        best_F1 = np.mean(sum_F1)\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"best threshold: {best_thresh}\")\n",
    "print(f\"best F1: {best_F1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiatoolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
