{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A place to test random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monkey.model.classification_model.efficientnet_b0 import (\n",
    "    EfficientNet_B0,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from monkey.model.utils import (\n",
    "    get_classification_metrics,\n",
    "    get_activation_function,\n",
    ")\n",
    "from monkey.model.loss_functions import get_loss_function, dice_coeff\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet_B0(input_channels=3, num_classes=1)\n",
    "model = model.eval()\n",
    "summary(model, input_size=(3, 32, 32))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(size=(4, 3, 32, 32), dtype=torch.float32)\n",
    "gt = torch.tensor([0, 0, 0, 1], dtype=torch.float)\n",
    "gt = torch.unsqueeze(gt, 1)\n",
    "\n",
    "dice_loss_fn = nn.BCELoss()\n",
    "\n",
    "pred_labels = []\n",
    "\n",
    "act = get_activation_function(\"sigmoid\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(test)\n",
    "    print(out.shape)\n",
    "    out = act(out)\n",
    "    # out = torch.squeeze(out)\n",
    "    pred = (out > 0.5).float()\n",
    "    pred = torch.squeeze(pred)\n",
    "    print(pred)\n",
    "    pred = pred.cpu().tolist()\n",
    "\n",
    "    pred_labels.extend(pred)\n",
    "\n",
    "    dice_loss = dice_loss_fn(out, gt)\n",
    "\n",
    "\n",
    "gt_labels = torch.squeeze(gt).cpu().tolist()\n",
    "\n",
    "print(f\"probs {out}\")\n",
    "print(out.shape)\n",
    "print(f\"gt {gt}\")\n",
    "print(gt.shape)\n",
    "print(f\"loss {dice_loss}\")\n",
    "# print(f\"weighted loss {weighted_loss}\")\n",
    "\n",
    "print(f\"pred labels: {pred_labels}\")\n",
    "print(f\"true labels: {gt_labels}\")\n",
    "\n",
    "metrics = get_classification_metrics(gt_labels, pred_labels)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mask = np.zeros(shape=(1, 2, 256, 256), dtype=np.float32)\n",
    "true_mask[0, 0, :, :] = 1\n",
    "true_mask = torch.tensor(\n",
    "    true_mask, dtype=torch.float32, requires_grad=True\n",
    ")\n",
    "\n",
    "# pprint(true_mask)\n",
    "pred_mask = np.zeros(shape=(1, 2, 256, 256), dtype=np.float32)\n",
    "pred_mask[0, 0, :, :] = 0.9\n",
    "pred_mask[0, 1, :, 0:128] = 0\n",
    "pred_mask = torch.tensor(\n",
    "    pred_mask, dtype=torch.float32, requires_grad=True\n",
    ")\n",
    "\n",
    "dice_loss_fn = get_loss_function(\"Dice\")\n",
    "dice_loss_fn.set_multiclass(True)\n",
    "\n",
    "dice_loss = dice_loss_fn.compute_loss(true_mask, pred_mask)\n",
    "\n",
    "pprint(dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_a = torch.zeros(size=(1, 1, 4, 4))\n",
    "mat_a[0, 0, 0:2, 0:2] = 1\n",
    "mat_b = torch.zeros(size=(1, 1, 4, 4))\n",
    "mat_b[0, 0, 0:2, 0:2] = 0.5\n",
    "epsilon = 1e-11\n",
    "non_zero_overlap = torch.dot(mat_a.reshape(-1), mat_b.reshape(-1))\n",
    "pprint(non_zero_overlap)\n",
    "sets_sum = torch.sum(mat_a) + torch.sum(mat_b)\n",
    "pprint(sets_sum)\n",
    "if non_zero_overlap == 0:\n",
    "    sets_sum = 1\n",
    "sim = (2 * non_zero_overlap + epsilon) / (sets_sum + epsilon)\n",
    "\n",
    "dice = dice_coeff(mat_a, mat_b, reduce_batch_first=True)\n",
    "pprint(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u1910100/miniconda3/envs/tiatoolbox/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "patch = np.zeros(shape=(32, 32, 3))\n",
    "patch = np.moveaxis(patch, 2, 0)\n",
    "patch = torch.tensor(patch, dtype=torch.float)\n",
    "patch = resize(patch, (224, 224))\n",
    "print(patch.shape)\n",
    "patch = torch.unsqueeze(patch, dim=0)\n",
    "\n",
    "\n",
    "print(patch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiatoolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
